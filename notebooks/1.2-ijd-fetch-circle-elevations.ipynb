{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Circle Elevations\n",
    "### Purpose\n",
    "In this notebook I query the USGS to get elevation data for all of the circles.\n",
    "This notebook addresses some one of the tasks in Github issue #35\n",
    "\n",
    "### Author: \n",
    "Ian Davis\n",
    "### Date: \n",
    "2020-03-31\n",
    "### Update Date: \n",
    "2020-05-03\n",
    "\n",
    "### Inputs \n",
    "1.1-circles_to_many_stations_usa_weather_data_20200424213015.csv - Comma separate file of the Christmas Bird Count and matches to 1 or more NOAA weather stations.\n",
    "- Data Dictonary can be found here: http://www.audubon.org/sites/default/files/documents/cbc_report_field_definitions_2013.pdf\n",
    "1.2-ijd-fetch-circle-elevations-OFFLINE.csv - Previously generated elevation data. This file will be used when you want to get the elevation data from an offline source and aoivd 100,000+ queries.\n",
    "\n",
    "### Output Files\n",
    "1.2-ijd-fetch-circle-elevations_20200502155633.csv - Only 1 column is added to the dataset, 'circle_elev'. This column is the elevation in meters for a given latitude and longitude of the circle centroid.\n",
    "\n",
    "## Steps or Proceedures in the notebook \n",
    "- Set runtime options\n",
    "    - Set option to retrieve elevations from offline source, or through the USGS queries\n",
    "    - Set option to only test the USGS query (NOTE: running the query function for the whole dataset will take 24+ hours)\n",
    "- Create a function to make a remote request to the USGS API\n",
    "- Create a function to supply inputs to the remote request and return the elevation value\n",
    "- Main sequence\n",
    "    - Read in dataset\n",
    "    - Create circl_elev column\n",
    "    - Loop through the dataset in chunks of 10000 to get elevation data\n",
    "    - (Optional) Retrieve elevations from offline data source instead of queries\n",
    "    - Write new dataset .txt file\n",
    "\n",
    "## References\n",
    "- elevation query: https://stackoverflow.com/questions/58350063/obtain-elevation-from-latitude-longitude-coordinates-with-a-simple-python-script\n",
    "- lamda functions: https://thispointer.com/python-how-to-use-if-else-elif-in-lambda-functions/\n",
    "- apply on Nulls: https://stackoverflow.com/questions/26614465/python-pandas-apply-function-if-a-column-value-is-not-null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import urllib\n",
    "import urllib3\n",
    "import time\n",
    "import gzip\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if you are running 32-bit Python (output would be False)\n",
    "# 32-bit Python could result in Memory Error when reading in large dataset\n",
    "import sys\n",
    "sys.maxsize > 2**32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set File Paths and Runtime Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to classify the name \n",
    "time_now = datetime.today().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "# File paths and script options\n",
    "PATH_TO_PAIRED_DATA = \"../data/Cloud_Data/1.1-circles_to_many_stations_usa_weather_data_20200424213015.csv\"\n",
    "PATH_TO_OFFLINE_ELEVATION_DATA = \"../data/Cloud_Data/1.2-ijd-fetch-circle-elevations-OFFLINE.csv\"\n",
    "PATH_TO_LOG_FILE = \"../data/Cloud_Data/1.2-ijd-fetch_circle_elevations_\"+time_now+\".log\"\n",
    "\n",
    "# option to pull offline elevation data from the /attic instead of running the queries\n",
    "get_offline_data = False\n",
    "\n",
    "# option to run a simple test of the query; only 1000 rows are queried instead of full dataset\n",
    "test_query = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Log File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not get_offline_data:\n",
    "    logging.basicConfig(filename=PATH_TO_LOG_FILE, \n",
    "                        filemode='w', \n",
    "                        format='%(message)s', \n",
    "                        level=logging.INFO)\n",
    "    logging.info('This log file shows the row index, lat, lon\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to make a remote request to the USGS API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_remote_request(url: str, params: dict):\n",
    "    \"\"\"\n",
    "    Makes the remote request\n",
    "    Continues making attempts until it succeeds\n",
    "    \"\"\"\n",
    "\n",
    "    count = 1\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get((url + urllib.parse.urlencode(params)))\n",
    "            time.sleep(1)\n",
    "        except (OSError, urllib3.exceptions.ProtocolError) as error:\n",
    "            logging.info('\\n')\n",
    "            logging.info('*' * 20, 'Error Occured', '*' * 20)\n",
    "            logging.info(f'Number of tries: {count}')\n",
    "            logging.info(f'URL: {url}')\n",
    "            logging.info(error)\n",
    "            logging.info('\\n')\n",
    "            count += 1\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to supply inputs to the remote request and return the elevation value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elevation_function(x):\n",
    "    \"\"\"\n",
    "    x - longitude\n",
    "    y - latitude\n",
    "    returns elevation in meters\n",
    "    \"\"\"\n",
    "    \n",
    "    url = 'https://nationalmap.gov/epqs/pqs.php?'\n",
    "    params = {'x': x[1],\n",
    "              'y': x[0],\n",
    "              'units': 'Meters',\n",
    "              'output': 'json'}\n",
    "    logging.info(str(x.name)+'\\t\\t'+str(x[0])+'\\t\\t'+str(x[1]))   # print row index, lat, lon\n",
    "    result = make_remote_request(url, params)\n",
    "    \n",
    "    return result.json()['USGS_Elevation_Point_Query_Service']['Elevation_Query']['Elevation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops to Query the USGS API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean to skip loop of queries and just pull elevation data from the \"attic\"\n",
    "if not get_offline_data:\n",
    "    \n",
    "    # load paired data file\n",
    "    #data_iterator = pd.read_csv(PATH_TO_TEMP_DATA, chunksize=1000, encoding = \"ISO-8859-1\", sep=\"\\t\")\n",
    "    data_iterator = pd.read_csv(PATH_TO_PAIRED_DATA, chunksize=1000, compression='gzip')\n",
    "        \n",
    "    chunk_list = []  \n",
    "    \n",
    "    # Each chunk is in dataframe format\n",
    "    for data_chunk in data_iterator:\n",
    "        # create elevation column\n",
    "        data_chunk.loc[:, 'circle_elev'] = np.nan\n",
    "        \n",
    "        # initial list of indices which are missing elevations\n",
    "        missing = data_chunk.loc[data_chunk['circle_elev'].isnull()].index\n",
    "    \n",
    "        # while loop to go over the dataset chunk times in the event that query requests fail\n",
    "        cnt=0 # counter to break while loop after\n",
    "        while len(missing) > 0:\n",
    "            if cnt == 5: break # exit while loop\n",
    "            cnt+=1\n",
    "            logging.info('while counter: '+str(cnt))\n",
    "        \n",
    "            try:\n",
    "                # combination of apply() function and lambda() function, only on nulls (see reference links above)\n",
    "                data_chunk.loc[:, 'circle_elev'] = data_chunk.loc[:, ['lat', 'lon', 'circle_elev']].apply(lambda x: elevation_function(x[0:2]) if(pd.isnull(x[2])) else x[2], axis=1)\n",
    "            except:\n",
    "                # on occasion query completely fails and crashes the function call\n",
    "                # problem is the stack prints to the notebook\n",
    "                # https://gist.github.com/wassname/d17325f36c36fa663dd7de3c09a55e74\n",
    "                #logging.error(\"Exception occurred\", exc_info=True)\n",
    "                logging.info(\"Exception occurred\")\n",
    "                continue\n",
    "    \n",
    "            # get new list of missing indices\n",
    "            missing = data_chunk.loc[data_chunk['circle_elev'].isnull()].index\n",
    "            # break the loop if there are no missing elevations\n",
    "            if len(missing) == 0: break\n",
    "        \n",
    "        # Append data chunk with elevation data to combined list\n",
    "        chunk_list.append(data_chunk)\n",
    "        \n",
    "        # Convert combined list into dataframe\n",
    "        filtered_data = pd.concat(chunk_list)\n",
    "        # Intermediate writes to .csv file so work is not lost in the event of code failure\n",
    "        filtered_data.to_csv(\"../data/Cloud_Data/1.2-ijd-fetch-circle-elevations_INT.csv\", sep=',', index=False)\n",
    "        del(filtered_data)\n",
    "        \n",
    "        # If just running a test, break the loop\n",
    "        if test_query: break\n",
    "    \n",
    "    # Convert combined list into dataframe\n",
    "    filtered_data = pd.concat(chunk_list)\n",
    "    \n",
    "    # close log file\n",
    "    #log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Get Elevation Data from Offline Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_offline_data:\n",
    "    # load offline data file\n",
    "    offline_data = pd.read_csv(PATH_TO_OFFLINE_ELEVATION_DATA)\n",
    "    \n",
    "    # convert count_date to string or merge won't match them properly \n",
    "    offline_data['count_date'] = pd.to_datetime(offline_data['count_date'])\n",
    "    \n",
    "    # round the latitude and longitudes to 4 digits\n",
    "    offline_data['lat'] = offline_data['lat'].round(3)\n",
    "    offline_data['lon'] = offline_data['lon'].round(3)\n",
    "    \n",
    "    # load paired data file\n",
    "    #data_iterator = pd.read_csv(PATH_TO_TEMP_DATA, chunksize=10000, encoding = \"ISO-8859-1\", sep=\"\\t\")\n",
    "    data_iterator = pd.read_csv(PATH_TO_PAIRED_DATA, \n",
    "                                compression='gzip', \n",
    "                                chunksize=10000)\n",
    "        \n",
    "    chunk_list = []  \n",
    "\n",
    "    # Each chunk is in dataframe format\n",
    "    print('The chunks should be the same length after merge.')\n",
    "    for data_chunk in data_iterator:\n",
    "        data_chunk['count_date'] = pd.to_datetime(data_chunk['count_date'])\n",
    "        data_chunk['lat'] = data_chunk['lat'].round(3)\n",
    "        data_chunk['lon'] = data_chunk['lon'].round(3)\n",
    "        filtered_chunk = pd.merge(data_chunk, \n",
    "                                  offline_data[['lat', 'lon', 'count_date', 'count_year', 'id', 'circle_elev']], \n",
    "                                  on=['lat', 'lon', 'count_date', 'count_year', 'id'],\n",
    "                                  how='left',\n",
    "                                  copy=False)\n",
    "        chunk_list.append(filtered_chunk)\n",
    "        print('Chunk Length Before: ', data_chunk.shape)\n",
    "        print('Chunk Length After: ', filtered_chunk.shape)\n",
    "\n",
    "    \n",
    "    filtered_data = pd.concat(chunk_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicates in offline data\n",
    "if get_offline_data:\n",
    "    offline_data.duplicated(subset=['lat', 'lon', 'count_date', 'count_year', 'id']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are there any duplicates in the data chunk?\n",
    "if get_offline_data:\n",
    "    data_chunk.duplicated(subset=['lat', 'lon', 'count_date', 'count_year', 'id']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screen Elevation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure elevations are the float\n",
    "filtered_data = pd.concat(chunk_list)\n",
    "filtered_data.loc[:, 'circle_elev'] = filtered_data.loc[:, 'circle_elev'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove bad elevation values\n",
    "filtered_data.loc[filtered_data['circle_elev'] < -10000.0, 'circle_elev'] = np.nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data[['lat', 'lon', 'count_date', 'circle_elev']].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram of elevations\n",
    "filtered_data.hist(column='circle_elev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same number of rows? Should be 109390\n",
    "len(filtered_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnamed columns\n",
    "# they were likely index columns auto-generated by pandas and then written to csv files, unintentionally\n",
    "filtered_data = filtered_data.loc[:, ~filtered_data.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dataframe on existing index\n",
    "filtered_data.sort_values(['int64_field_0'], ascending=[True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_offline_data:\n",
    "    print('If from an offline source, check to make sure circle elevations are not being lost during merge:\\n')\n",
    "    print('NA in Merged:\\n', filtered_data['circle_elev'].isna().value_counts())\n",
    "    print('\\n')\n",
    "    print('NA in Offline:\\n', offline_data['circle_elev'].isna().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing elevations:')\n",
    "filtered_data['circle_elev'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('How many elevations at sea level?')\n",
    "filtered_data.loc[filtered_data['circle_elev'] == 0.0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.to_csv(\"../data/Cloud_Data/1.2-ijd-fetch-circle-elevations_\"+time_now+\".csv\", \n",
    "                     sep=',', \n",
    "                     compression='gzip',\n",
    "                     index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
